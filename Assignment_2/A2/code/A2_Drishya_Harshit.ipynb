{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44ac242",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f44ac242",
    "outputId": "ceb23d09-c018-4f32-eec9-156261f56d53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk  \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.metrics.distance  import edit_distance\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "OH9GUkkc_peq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH9GUkkc_peq",
    "outputId": "0fd1608a-29d6-4aa8-96dd-49f075ae0d2a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05df5da6",
   "metadata": {
    "id": "05df5da6"
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/content/drive/MyDrive/A1_dataset.csv')\n",
    "data = pd.read_csv('C:/Users/HP/Downloads/A1_dataset.csv')\n",
    "test_data = pd.read_csv('C:/Users/HP/Downloads/A2_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "CXVRoY9I7KZj",
   "metadata": {
    "id": "CXVRoY9I7KZj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def text_cleaner(text):    \n",
    "    # lower case text\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    \n",
    "    # remove punctuations\n",
    "    newString = re.sub(r'http\\S+', '', newString)\n",
    "    newString1 = re.sub(\"[^a-zA-Z]\", \" \", newString)    \n",
    "    newString2 = re.sub(r'\\d+', '', newString1)\n",
    "    \n",
    "    #return newString\n",
    "    long_words=[]\n",
    "    # remove short word\n",
    "    stopword = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s?')\n",
    "    text1 = stopword.sub(r'',str(newString2))\n",
    "\n",
    "    for i in text1.split():\n",
    "        if len(i)>=3:   \n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "FJw0VXCfNExl",
   "metadata": {
    "id": "FJw0VXCfNExl"
   },
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "data['new'] = data['TEXT'].apply(lambda x : text_cleaner(x))\n",
    "#data_new = text_cleaner(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6zFw8Dp4zbGi",
   "metadata": {
    "id": "6zFw8Dp4zbGi"
   },
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "test_data['new'] = test_data['TEXT'].apply(lambda x : text_cleaner(x))\n",
    "#data_new = text_cleaner(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "zFfbofeSzg8C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "zFfbofeSzg8C",
    "outputId": "85de2b4d-d3af-4786-a369-39cd25134a98"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri May 29 22:24:26 2009</td>\n",
       "      <td>@mileycyrus cheer up miley whats wrong?</td>\n",
       "      <td>mileycyrus cheer miley whats wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jun 07 01:37:36 2009</td>\n",
       "      <td>Just got back in from The Belcourt. Saw &amp;quot;...</td>\n",
       "      <td>got back belcourt saw quot fifth element quot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Wed May 13 23:41:18 2009</td>\n",
       "      <td>http://bit.ly/IQPPD  with video</td>\n",
       "      <td>video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun May 31 16:43:58 2009</td>\n",
       "      <td>@chloebli heyy!  how was your carnavilistic da...</td>\n",
       "      <td>chloebli heyy carnavilistic day woow made word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri May 29 10:36:59 2009</td>\n",
       "      <td>@deadlyseagal http://twitpic.com/66zex - Nice ...</td>\n",
       "      <td>deadlyseagal nice day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jun 07 12:52:53 2009</td>\n",
       "      <td>@jakesonaplane ha yup  its gonna be a good day</td>\n",
       "      <td>jakesonaplane yup gonna good day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun May 03 20:00:34 2009</td>\n",
       "      <td>@DavidArchie Hey Love the episode but why only...</td>\n",
       "      <td>davidarchie hey love episode mins awesome loved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jun 01 05:14:30 2009</td>\n",
       "      <td>Sick--or allergytastic (not sure which!)--just...</td>\n",
       "      <td>sick allergytastic sure time miss saigon audit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jun 07 08:16:06 2009</td>\n",
       "      <td>headache  ... but sooooo worth it haha</td>\n",
       "      <td>headache sooooo worth haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1</td>\n",
       "      <td>Tue Jun 02 05:33:31 2009</td>\n",
       "      <td>Revising Byron in me garden, it's well hot!! C...</td>\n",
       "      <td>revising byron garden well hot canto amp done ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>644 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LABEL                 DATE_TIME  \\\n",
       "0        1  Fri May 29 22:24:26 2009   \n",
       "1        1  Sun Jun 07 01:37:36 2009   \n",
       "2        1  Wed May 13 23:41:18 2009   \n",
       "3        1  Sun May 31 16:43:58 2009   \n",
       "4        1  Fri May 29 10:36:59 2009   \n",
       "..     ...                       ...   \n",
       "639      1  Sun Jun 07 12:52:53 2009   \n",
       "640      0  Sun May 03 20:00:34 2009   \n",
       "641      0  Mon Jun 01 05:14:30 2009   \n",
       "642      0  Sun Jun 07 08:16:06 2009   \n",
       "643      1  Tue Jun 02 05:33:31 2009   \n",
       "\n",
       "                                                  TEXT  \\\n",
       "0             @mileycyrus cheer up miley whats wrong?    \n",
       "1    Just got back in from The Belcourt. Saw &quot;...   \n",
       "2                     http://bit.ly/IQPPD  with video    \n",
       "3    @chloebli heyy!  how was your carnavilistic da...   \n",
       "4    @deadlyseagal http://twitpic.com/66zex - Nice ...   \n",
       "..                                                 ...   \n",
       "639     @jakesonaplane ha yup  its gonna be a good day   \n",
       "640  @DavidArchie Hey Love the episode but why only...   \n",
       "641  Sick--or allergytastic (not sure which!)--just...   \n",
       "642             headache  ... but sooooo worth it haha   \n",
       "643  Revising Byron in me garden, it's well hot!! C...   \n",
       "\n",
       "                                                   new  \n",
       "0                   mileycyrus cheer miley whats wrong  \n",
       "1    got back belcourt saw quot fifth element quot ...  \n",
       "2                                                video  \n",
       "3       chloebli heyy carnavilistic day woow made word  \n",
       "4                                deadlyseagal nice day  \n",
       "..                                                 ...  \n",
       "639                   jakesonaplane yup gonna good day  \n",
       "640    davidarchie hey love episode mins awesome loved  \n",
       "641  sick allergytastic sure time miss saigon audit...  \n",
       "642                         headache sooooo worth haha  \n",
       "643  revising byron garden well hot canto amp done ...  \n",
       "\n",
       "[644 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "I4BN55IqNTeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "I4BN55IqNTeb",
    "outputId": "056b572d-2e50-47db-b413-30e2534495ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri Jun 05 14:26:50 2009</td>\n",
       "      <td>About to get threaded and scared</td>\n",
       "      <td>get threaded scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thu May 14 10:13:55 2009</td>\n",
       "      <td>@awaisnaseer I like Shezan Mangooo too!!! I ha...</td>\n",
       "      <td>awaisnaseer like shezan mangooo one yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri Jun 05 21:02:20 2009</td>\n",
       "      <td>worked on my car after work. showering then go...</td>\n",
       "      <td>worked car work showering going bed sooooooooo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jun 14 22:25:52 2009</td>\n",
       "      <td>@Marama Actually we start this afternoon!  I w...</td>\n",
       "      <td>marama actually start afternoon try something ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun May 31 00:42:12 2009</td>\n",
       "      <td>@gfalcone601 Aww Gi.don't worry.we'll vote for...</td>\n",
       "      <td>gfalcone aww worry vote non stop coz love much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>1</td>\n",
       "      <td>Sat Jun 06 22:45:26 2009</td>\n",
       "      <td>@QandQ My performances on my CLEP tests.  #qshock</td>\n",
       "      <td>qandq performances clep tests qshock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>0</td>\n",
       "      <td>Tue Jun 16 10:17:07 2009</td>\n",
       "      <td>ugh no, rcn had all the true blood episodes on...</td>\n",
       "      <td>ugh rcn true blood episodes demand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri May 01 22:00:42 2009</td>\n",
       "      <td>Just returned from the forest! Sarah (my merch...</td>\n",
       "      <td>returned forest sarah merchy lost keys woods day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jun 07 02:09:46 2009</td>\n",
       "      <td>is proud of her dad and his piece of work. ( h...</td>\n",
       "      <td>proud dad piece work keep papa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri May 22 04:49:37 2009</td>\n",
       "      <td>Just woke up, gonna eat pizza for breakfast. A...</td>\n",
       "      <td>woke gonna eat pizza breakfast also dentist ap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4287 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LABEL                 DATE_TIME  \\\n",
       "0         0  Fri Jun 05 14:26:50 2009   \n",
       "1         1  Thu May 14 10:13:55 2009   \n",
       "2         1  Fri Jun 05 21:02:20 2009   \n",
       "3         1  Sun Jun 14 22:25:52 2009   \n",
       "4         1  Sun May 31 00:42:12 2009   \n",
       "...     ...                       ...   \n",
       "4282      1  Sat Jun 06 22:45:26 2009   \n",
       "4283      0  Tue Jun 16 10:17:07 2009   \n",
       "4284      1  Fri May 01 22:00:42 2009   \n",
       "4285      1  Sun Jun 07 02:09:46 2009   \n",
       "4286      0  Fri May 22 04:49:37 2009   \n",
       "\n",
       "                                                   TEXT  \\\n",
       "0                     About to get threaded and scared    \n",
       "1     @awaisnaseer I like Shezan Mangooo too!!! I ha...   \n",
       "2     worked on my car after work. showering then go...   \n",
       "3     @Marama Actually we start this afternoon!  I w...   \n",
       "4     @gfalcone601 Aww Gi.don't worry.we'll vote for...   \n",
       "...                                                 ...   \n",
       "4282  @QandQ My performances on my CLEP tests.  #qshock   \n",
       "4283  ugh no, rcn had all the true blood episodes on...   \n",
       "4284  Just returned from the forest! Sarah (my merch...   \n",
       "4285  is proud of her dad and his piece of work. ( h...   \n",
       "4286  Just woke up, gonna eat pizza for breakfast. A...   \n",
       "\n",
       "                                                    new  \n",
       "0                                   get threaded scared  \n",
       "1         awaisnaseer like shezan mangooo one yesterday  \n",
       "2     worked car work showering going bed sooooooooo...  \n",
       "3     marama actually start afternoon try something ...  \n",
       "4        gfalcone aww worry vote non stop coz love much  \n",
       "...                                                 ...  \n",
       "4282               qandq performances clep tests qshock  \n",
       "4283                 ugh rcn true blood episodes demand  \n",
       "4284   returned forest sarah merchy lost keys woods day  \n",
       "4285                     proud dad piece work keep papa  \n",
       "4286  woke gonna eat pizza breakfast also dentist ap...  \n",
       "\n",
       "[4287 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sVcmB2l1KLbz",
   "metadata": {
    "id": "sVcmB2l1KLbz"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "F_8wj1VkC0zJ",
   "metadata": {
    "id": "F_8wj1VkC0zJ"
   },
   "outputs": [],
   "source": [
    "a = data['new'].to_list()\n",
    "np.savetxt(r'C:/Users/HP/Downloads/b.txt', a,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664468c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete Bigram Language Model\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class Ngram:\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.bigram_prob = None\n",
    "\n",
    "        # BIGRAM GOOD TURING RESULTS\n",
    "        self.bigram_good_turing = None\n",
    "        self.bigram_zero_occurence_prob = None\n",
    "        self.bigram_good_turing_cstar = None\n",
    "\n",
    "\n",
    "    def train(self, sentences):\n",
    "        listOfBigrams = []\n",
    "        bigramCounts = {}\n",
    "        unigramCounts = {}\n",
    "        listOfTrigrams = []\n",
    "        trigramCounts = {}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for i in range(len(words)):\n",
    "                if i < len(words) - 1:\n",
    "                    listOfBigrams.append((words[i], words[i + 1]))\n",
    "                    if (words[i], words[i + 1]) in bigramCounts:\n",
    "                        bigramCounts[(words[i], words[i + 1])] += 1\n",
    "                    else:\n",
    "                        bigramCounts[(words[i], words[i + 1])] = 1\n",
    "\n",
    "                    if i < len(words) - 2:\n",
    "                        listOfTrigrams.append((words[i], words[i + 1], words[i + 2]))\n",
    "                        if (words[i], words[i + 1], words[i + 2]) in trigramCounts:\n",
    "                            trigramCounts[(words[i], words[i + 1], words[i + 2])] += 1\n",
    "                        else:\n",
    "                            trigramCounts[(words[i], words[i + 1], words[i + 2])] = 1\n",
    "\n",
    "                if words[i] in unigramCounts:\n",
    "                    unigramCounts[words[i]] += 1\n",
    "                else:\n",
    "                    unigramCounts[words[i]] = 1\n",
    "\n",
    "        return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "\n",
    "    def get_bigram_prob(self, sentence):\n",
    "            '''Return bigram probabilty of the sentence'''\n",
    "            score = 1\n",
    "            words = sentence.split()\n",
    "            for i in range(len(words) - 1):\n",
    "                unit = (words[i], words[i + 1])\n",
    "                if unit in self.bigram_prob:\n",
    "                    score *= self.bigram_prob[unit]\n",
    "                else:\n",
    "                    score *= 0\n",
    "            return score\n",
    "\n",
    "    def get_bigram_good_turing_prob(self, sentence):\n",
    "        '''Return bigram probabilty of the sentence with good turing smoohting'''\n",
    "        score = 1\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            unit = (words[i], words[i + 1])\n",
    "            if unit in self.bigram_good_turing:\n",
    "                score *= self.bigram_good_turing[unit]\n",
    "            else:\n",
    "                score *= self.bigram_zero_occurence_prob\n",
    "        return score\n",
    "\n",
    "    def prob(self, sentence):\n",
    "        '''Returns the MLE probability of the given sentence. '''\n",
    "        if self.N == 2:\n",
    "            return self.get_bigram_prob(sentence)\n",
    "\n",
    "\n",
    "    def sprob(self, sentence):\n",
    "\n",
    "        '''Returns the smoothed probability of a given sentence. '''\n",
    "        if self.N == 2:\n",
    "            return self.get_bigram_good_turing_prob(sentence)\n",
    "\n",
    "\n",
    "    def next(self, before_prior=None, prior=None):\n",
    "        '''Samples a word from the conditional distribution of given context.'''\n",
    "        if self.N == 2:\n",
    "            return self.bigram_generate_next_word(prior)\n",
    "\n",
    "    \n",
    "    def calculate_bigram_prob(self, listOfBigrams, unigramCounts, bigramCounts):\n",
    "        '''Assigns bigram probabilties'''\n",
    "        bigram_prob = {}\n",
    "        for bigram in listOfBigrams:\n",
    "            bigram_prob[bigram] = (bigramCounts.get(bigram)) / (unigramCounts.get(bigram[0]))\n",
    "\n",
    "        with open('./2gram.txt', 'w') as file:\n",
    "            file.write('Bigram'  + 'Count' +  'Probability' + '\\n')\n",
    "            for bigrams in listOfBigrams:\n",
    "                file.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams]) + ' : ' + str(bigram_prob[bigrams]) + '\\n')\n",
    "        self.bigram_prob = bigram_prob\n",
    "        return bigram_prob\n",
    "    \n",
    "    \n",
    "    def calculate_bigram_prob_positive_negative(self, listOfBigrams, unigramCounts, bigramCounts, positive_negative, beta_component):\n",
    "        '''Assigns bigram probabilties'''\n",
    "        import math\n",
    "        bigram_prob = {}\n",
    "        \n",
    "        for bigram in listOfBigrams:\n",
    "            if positive_negative == 0:\n",
    "                bigram_prob[bigram] = ((bigramCounts.get(bigram)) / (unigramCounts.get(bigram[0]))) / math.log(beta_component)\n",
    "            else:\n",
    "                bigram_prob[bigram] = ((bigramCounts.get(bigram)) / (unigramCounts.get(bigram[0]))) / (2 * math.log(beta_component))\n",
    "        self.bigram_prob = bigram_prob\n",
    "        return bigram_prob\n",
    "\n",
    "\n",
    "    def good_turing_smooting(self, list_of_word_unit, word_unit_counts, total_number_of_word_unit, filename_debug, filename_result):\n",
    "        '''Assigns good turing smoothed probabilties'''\n",
    "        list_of_probabilities = {}\n",
    "        bucket = {}\n",
    "        c_star = {}\n",
    "        p_star = {}\n",
    "        list_of_counts = {}\n",
    "\n",
    "        for word_unit in word_unit_counts.items():\n",
    "            value = word_unit[1]  # Of time occurs\n",
    "            if not value in bucket:\n",
    "                bucket[value] = 1\n",
    "            else:\n",
    "                bucket[value] += 1\n",
    "\n",
    "        bucket_list = sorted(bucket.items(), key=lambda t: t[0])\n",
    "        zero_occurence_prob = bucket_list[0][1] / total_number_of_word_unit  # This many words occurs only once\n",
    "        last_item = bucket_list[len(bucket_list) - 1][0]  # Most 266 occurs, occurs only one time\n",
    "\n",
    "        # Set non existing # of words\n",
    "        for x in range(1, last_item):\n",
    "            if x not in bucket:\n",
    "                bucket[x] = 0\n",
    "\n",
    "        bucket_list = sorted(bucket.items(), key=lambda t: t[0])\n",
    "        bucket_list_len = len(bucket_list)\n",
    "\n",
    "        i = 1\n",
    "        file = open(filename_debug, 'w')\n",
    "        file.write(\"#NumberOfOccurences\\t\\t\\tFrequency\\n\")\n",
    "        for k, v in bucket_list:\n",
    "            file.write(str(k) + \" : \" + str(v) + \"\\n\")\n",
    "            if i < bucket_list_len:\n",
    "                if v == 0:\n",
    "                    c_star[k] = 0\n",
    "                    p_star[k] = 0\n",
    "\n",
    "                else:\n",
    "                    c_star[k] = (i + 1) * bucket_list[i][1] / v\n",
    "                    p_star[k] = c_star[k] / total_number_of_word_unit\n",
    "\n",
    "            else:\n",
    "                c_star[k] = 0\n",
    "                p_star[k] = 0\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        file.close()\n",
    "        for word_unit in list_of_word_unit:\n",
    "            list_of_probabilities[word_unit] = p_star.get(word_unit_counts[word_unit])\n",
    "            list_of_counts[word_unit] = c_star.get(word_unit_counts[word_unit])\n",
    "\n",
    "        with open(filename_result, 'w') as file:\n",
    "            file.write('Word Unit' +':'+ 'Count' + 'Probability' + '\\n')\n",
    "\n",
    "            for bigrams in list_of_word_unit:\n",
    "                file.write(str(bigrams) + ' : ' + str(word_unit_counts[bigrams])\n",
    "                           + ' : ' + str(list_of_probabilities[bigrams]) + '\\n')\n",
    "\n",
    "        return list_of_probabilities, zero_occurence_prob, list_of_counts\n",
    "\n",
    "    def bigram_generate_next_word(self, prior, pos_neg=0):\n",
    "        '''Genereates new word - bigram'''\n",
    "        choose_list = []\n",
    "        for elem in list(self.bigram_prob.keys()):\n",
    "            if elem[0] == prior:\n",
    "                choose_list.append(elem[1])\n",
    "        while True:\n",
    "            return random.choice(choose_list)\n",
    "\n",
    "    def bigram_generate_sentence(self, maximum_iteration=7):\n",
    "        '''Genereates a sentence using bigrams'''\n",
    "        for i in range(7):\n",
    "            sent = []\n",
    "            word = \"<s>\"\n",
    "            sent.append(word)\n",
    "            for index in range(0, maximum_iteration):\n",
    "                word = self.bigram_generate_next_word(word, 1)\n",
    "                sent.append(word)\n",
    "                if word == \"</s>\":\n",
    "                    break\n",
    "            if index == maximum_iteration - 1:\n",
    "                sent.append(\"</s>\")\n",
    "            return sent\n",
    "\n",
    "    def bigram_generate_sentences(self, number_of_sentences, maximum_iteration=7):\n",
    "        '''Genereates some sentences using bigrams'''\n",
    "        for i in range(7):\n",
    "            sentences = []\n",
    "            for i in range(0, number_of_sentences):\n",
    "                sentence = self.bigram_generate_sentence(maximum_iteration)\n",
    "                sentences.append(sentence)\n",
    "            return sentences\n",
    "\n",
    "    def ppl_bigram(self, sentences):\n",
    "        '''Returns the bigram perplexity of the given list of sentences. Here smoothed values are used.'''\n",
    "        counter = 0\n",
    "        tmp = 0\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            for j in range(len(sentence) - 1):\n",
    "                unit = (sentence[j], sentence[j + 1])\n",
    "                if unit in self.bigram_prob:\n",
    "                    prob = self.bigram_prob[unit]\n",
    "                else:\n",
    "                    prob = 0\n",
    "                if prob == 0:\n",
    "                    tmp += 0\n",
    "                else:\n",
    "                    tmp += math.log(prob, 2)\n",
    "                counter += 1\n",
    "        # entropy = prob of each token / number of tokens\n",
    "        try:\n",
    "            entropy = -1 / counter * tmp\n",
    "        except ZeroDivisionError:\n",
    "            entropy = 0\n",
    "        \n",
    "        perplexity = math.pow(2, entropy)\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Sl2NliIAeFUu",
   "metadata": {
    "id": "Sl2NliIAeFUu"
   },
   "outputs": [],
   "source": [
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        all_lines = file.readlines()\n",
    "\n",
    "    return_lines = []\n",
    "    for line in all_lines:\n",
    "        return_lines.append(preprocess(line))\n",
    "\n",
    "    return return_lines\n",
    "\n",
    "\n",
    "def preprocess(line):\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    line = START + \" \" + line + \" \" + END\n",
    "    #line = \" \" +line + \" \"\n",
    "    return line\n",
    "\n",
    "\n",
    "def process_test_data(sentences):\n",
    "    return_list = []\n",
    "    for sentence in sentences:\n",
    "        return_list.append(sentence.split())\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "jbNEz_mmmoic",
   "metadata": {
    "id": "jbNEz_mmmoic"
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "\n",
    "# Will be using 2 LM models for finding the average of positive and negative accuracies\n",
    "\n",
    "def vader_analysis(data: pd.DataFrame, positive_negative: int):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    data['scores'] = data['sentence'].apply(lambda review: sid.polarity_scores(review))\n",
    "    data['compound']  = data['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "    data['comp_score'] = data['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n",
    "    polarity = data['comp_score'].tolist()\n",
    "    c = Counter(polarity)\n",
    "    if positive_negative == 0:\n",
    "        return data, c['neg']\n",
    "    else:\n",
    "        return data, c['pos']\n",
    "    \n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "PcUldYgXmoid",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcUldYgXmoid",
    "outputId": "f94ab613-8d18-4c06-da4b-306bcb1323a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING STARTED...\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "For beta_component =  2\n",
      "Count of correct negative sentences generated =  54\n",
      "Count of correct positive sentences generated =  196\n",
      "Accuracy =  0.5\n",
      "For beta_component =  3\n",
      "Count of correct negative sentences generated =  62\n",
      "Count of correct positive sentences generated =  199\n",
      "Accuracy =  0.522\n",
      "For beta_component =  4\n",
      "Count of correct negative sentences generated =  53\n",
      "Count of correct positive sentences generated =  206\n",
      "Accuracy =  0.518\n",
      "For beta_component =  5\n",
      "Count of correct negative sentences generated =  52\n",
      "Count of correct positive sentences generated =  199\n",
      "Accuracy =  0.502\n",
      "For beta_component =  6\n",
      "Count of correct negative sentences generated =  63\n",
      "Count of correct positive sentences generated =  199\n",
      "Accuracy =  0.524\n",
      "For beta_component =  7\n",
      "Count of correct negative sentences generated =  48\n",
      "Count of correct positive sentences generated =  198\n",
      "Accuracy =  0.492\n",
      "For beta_component =  8\n",
      "Count of correct negative sentences generated =  45\n",
      "Count of correct positive sentences generated =  191\n",
      "Accuracy =  0.472\n",
      "For beta_component =  9\n",
      "Count of correct negative sentences generated =  56\n",
      "Count of correct positive sentences generated =  206\n",
      "Accuracy =  0.524\n",
      "For beta_component =  10\n",
      "Count of correct negative sentences generated =  52\n",
      "Count of correct positive sentences generated =  194\n",
      "Accuracy =  0.492\n",
      "For beta_component =  11\n",
      "Count of correct negative sentences generated =  57\n",
      "Count of correct positive sentences generated =  190\n",
      "Accuracy =  0.494\n",
      "For beta_component =  12\n",
      "Count of correct negative sentences generated =  60\n",
      "Count of correct positive sentences generated =  192\n",
      "Accuracy =  0.504\n",
      "For beta_component =  13\n",
      "Count of correct negative sentences generated =  55\n",
      "Count of correct positive sentences generated =  200\n",
      "Accuracy =  0.51\n",
      "For beta_component =  14\n",
      "Count of correct negative sentences generated =  49\n",
      "Count of correct positive sentences generated =  188\n",
      "Accuracy =  0.474\n",
      "For beta_component =  15\n",
      "Count of correct negative sentences generated =  51\n",
      "Count of correct positive sentences generated =  202\n",
      "Accuracy =  0.506\n",
      "For beta_component =  16\n",
      "Count of correct negative sentences generated =  55\n",
      "Count of correct positive sentences generated =  200\n",
      "Accuracy =  0.51\n",
      "For beta_component =  17\n",
      "Count of correct negative sentences generated =  51\n",
      "Count of correct positive sentences generated =  206\n",
      "Accuracy =  0.514\n",
      "For beta_component =  18\n",
      "Count of correct negative sentences generated =  50\n",
      "Count of correct positive sentences generated =  197\n",
      "Accuracy =  0.494\n",
      "For beta_component =  19\n",
      "Count of correct negative sentences generated =  71\n",
      "Count of correct positive sentences generated =  197\n",
      "Accuracy =  0.536\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_filename = 'C:/Users/HP/Downloads/b.txt'\n",
    "train_data = read_data(train_filename)\n",
    "ngram = Ngram(2)\n",
    "print(\"TRAINING STARTED...\")\n",
    "list_of_bigrams, unigram_counts, bigram_counts = ngram.train(train_data)\n",
    "bigram_prob = ngram.calculate_bigram_prob(list_of_bigrams, unigram_counts, bigram_counts)\n",
    "#one_gram_add_one_prob = ngram.onegram_add_one_smothing(unigram_counts)\n",
    "ngram.bigram_good_turing, ngram.bigram_zero_occurence_prob, ngram.bigram_good_turing_cstar = ngram.good_turing_smooting(list_of_bigrams, bigram_counts, len(list_of_bigrams),\n",
    "                                                                                                                            'C:/Users/HP/Downloads/good_turing_smooting_bigram.txt',\n",
    "                                                                                                                            'C:/Users/HP/Downloads/good_turing_smooting_bigram_result.txt')\n",
    "beta_components = list(range(2, 20))\n",
    "print(beta_components)\n",
    "max_accuracy = 0\n",
    "best_positive = []\n",
    "best_negative = []\n",
    "\n",
    "for beta_component in beta_components:\n",
    "    print(\"For beta_component = \", beta_component)\n",
    "    # Working the negative part\n",
    "    bigram_prob_pos_neg = ngram.calculate_bigram_prob_positive_negative(list_of_bigrams, unigram_counts, bigram_counts, 0, beta_component)\n",
    "    bigram_sentences = ngram.bigram_generate_sentences(250)\n",
    "    negative_sentences = []\n",
    "    for sentence in bigram_sentences:\n",
    "        joined_sentence = \" \".join(sentence)\n",
    "        negative_sentences.append(joined_sentence)\n",
    "    negative_data = pd.DataFrame(data=negative_sentences, columns=['sentence'])\n",
    "    negative_data, negative_count = vader_analysis(negative_data, 0)\n",
    "    print(\"Count of correct negative sentences generated = \", negative_count)\n",
    "    \n",
    "    # Working the positive part\n",
    "    bigram_prob_pos_neg = ngram.calculate_bigram_prob_positive_negative(list_of_bigrams, unigram_counts, bigram_counts, 1, beta_component)\n",
    "    bigram_sentences = ngram.bigram_generate_sentences(250)\n",
    "    positive_sentences = []\n",
    "    for sentence in bigram_sentences:\n",
    "        joined_sentence = \" \".join(sentence)\n",
    "        positive_sentences.append(joined_sentence)\n",
    "    positive_data = pd.DataFrame(data=positive_sentences, columns=['sentence'])\n",
    "    positive_data, positive_count = vader_analysis(positive_data, 1)\n",
    "    print(\"Count of correct positive sentences generated = \", positive_count)\n",
    "    \n",
    "    positive_accuracy = (float)(positive_count/250)\n",
    "    negative_accuracy = (float)(negative_count/250)\n",
    "    accuracy = (float)((positive_accuracy+negative_accuracy)/2)\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        best_positive = positive_data\n",
    "        best_negative = negative_data\n",
    "    print(\"Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "l1Jzhco4hjOM",
   "metadata": {
    "id": "l1Jzhco4hjOM"
   },
   "outputs": [],
   "source": [
    "import dill as pickle \n",
    "with open('a2_ngram_model.pkl', 'wb') as fout:\n",
    "    pickle.dump(ngram, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vP-uV-J4hjLq",
   "metadata": {
    "id": "vP-uV-J4hjLq"
   },
   "outputs": [],
   "source": [
    "with open('a2_ngram_model.pkl', 'rb') as fin:\n",
    "    ngram_loaded = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wquVbe02c7BV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wquVbe02c7BV",
    "outputId": "85f66203-8e8f-4f53-e5da-c59aa156e346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "p = best_negative['sentence']\n",
    "print(ngram.ppl_bigram(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01M2PuJuilph",
   "metadata": {
    "id": "01M2PuJuilph"
   },
   "outputs": [],
   "source": [
    "best_positive['sentence'] = best_positive['sentence'] .map(lambda x: x.lstrip('<s>').rstrip('</s>'))\n",
    "best_negative['sentence'] = best_negative['sentence'] .map(lambda x: x.lstrip('<s>').rstrip('</s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "WxHJfc7zcV4D",
   "metadata": {
    "id": "WxHJfc7zcV4D"
   },
   "outputs": [],
   "source": [
    "a = best_positive['sentence']\n",
    "b = best_negative['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "wIXXPyz5cpLl",
   "metadata": {
    "id": "wIXXPyz5cpLl"
   },
   "outputs": [],
   "source": [
    "df = [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rJv_XPNYWJPF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJv_XPNYWJPF",
    "outputId": "3d4b6bc5-a310-4c9f-f289-baa4c2cc765d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "bigram_ppl_score = ngram.ppl_bigram(df)\n",
    "print(bigram_ppl_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "I-bqVvwnrB96",
   "metadata": {
    "id": "I-bqVvwnrB96"
   },
   "outputs": [],
   "source": [
    "best_negative['LABEL'] = best_negative['comp_score'].map({'pos': 1, 'neg':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "P-etmtjyrB5j",
   "metadata": {
    "id": "P-etmtjyrB5j"
   },
   "outputs": [],
   "source": [
    "best_positive['LABEL'] = best_positive['comp_score'].map({'pos': 1, 'neg':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "BrAMzDY4rolK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "BrAMzDY4rolK",
    "outputId": "ed740fcd-042a-4996-81a4-bd4260a9e44c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>comp_score</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>godlessgirl add none want shopping eating toast</td>\n",
       "      <td>{'neg': 0.133, 'neu': 0.867, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>madirigh damn sucks gotta</td>\n",
       "      <td>{'neg': 0.565, 'neu': 0.435, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.6369</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idk tell girl shut like thunder woke</td>\n",
       "      <td>{'neg': 0.128, 'neu': 0.642, 'pos': 0.229, 'co...</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>terilynns amazing book school tommorow</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.612, 'pos': 0.388, 'comp...</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>totzhatz sometimes late tired chill</td>\n",
       "      <td>{'neg': 0.326, 'neu': 0.674, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>lab work best massage</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.543, 'pos': 0.457, 'comp...</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>chinese fooood mmmmm</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>leeboardman cheers follow anyways</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.617, 'pos': 0.383, 'comp...</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>diner guess wrong smile god little bit</td>\n",
       "      <td>{'neg': 0.226, 'neu': 0.438, 'pos': 0.336, 'co...</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>maynardcomau boy see done homework vain exist...</td>\n",
       "      <td>{'neg': 0.259, 'neu': 0.741, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "0     godlessgirl add none want shopping eating toast    \n",
       "1                           madirigh damn sucks gotta    \n",
       "2                idk tell girl shut like thunder woke    \n",
       "3              terilynns amazing book school tommorow    \n",
       "4                 totzhatz sometimes late tired chill    \n",
       "..                                                 ...   \n",
       "245                             lab work best massage    \n",
       "246                              chinese fooood mmmmm    \n",
       "247                 leeboardman cheers follow anyways    \n",
       "248            diner guess wrong smile god little bit    \n",
       "249   maynardcomau boy see done homework vain exist...   \n",
       "\n",
       "                                                scores  compound comp_score  \\\n",
       "0    {'neg': 0.133, 'neu': 0.867, 'pos': 0.0, 'comp...   -0.0572        neg   \n",
       "1    {'neg': 0.565, 'neu': 0.435, 'pos': 0.0, 'comp...   -0.6369        neg   \n",
       "2    {'neg': 0.128, 'neu': 0.642, 'pos': 0.229, 'co...    0.2732        pos   \n",
       "3    {'neg': 0.0, 'neu': 0.612, 'pos': 0.388, 'comp...    0.5859        pos   \n",
       "4    {'neg': 0.326, 'neu': 0.674, 'pos': 0.0, 'comp...   -0.4404        neg   \n",
       "..                                                 ...       ...        ...   \n",
       "245  {'neg': 0.0, 'neu': 0.543, 'pos': 0.457, 'comp...    0.6369        pos   \n",
       "246  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000        pos   \n",
       "247  {'neg': 0.0, 'neu': 0.617, 'pos': 0.383, 'comp...    0.4767        pos   \n",
       "248  {'neg': 0.226, 'neu': 0.438, 'pos': 0.336, 'co...    0.1280        pos   \n",
       "249  {'neg': 0.259, 'neu': 0.741, 'pos': 0.0, 'comp...   -0.4215        neg   \n",
       "\n",
       "     LABEL  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  \n",
       "3        1  \n",
       "4        0  \n",
       "..     ...  \n",
       "245      1  \n",
       "246      1  \n",
       "247      1  \n",
       "248      1  \n",
       "249      0  \n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e8H-uuZrpDL",
   "metadata": {
    "id": "9e8H-uuZrpDL"
   },
   "outputs": [],
   "source": [
    "best_negative = best_negative.drop(['scores'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ktlyEOrrpA8",
   "metadata": {
    "id": "8ktlyEOrrpA8"
   },
   "outputs": [],
   "source": [
    "best_negative = best_negative.drop(['compound'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hGeflRP3rkyj",
   "metadata": {
    "id": "hGeflRP3rkyj"
   },
   "outputs": [],
   "source": [
    "best_negative = best_negative.drop(['comp_score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mBygavWnsC2R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "mBygavWnsC2R",
    "outputId": "5cb90e22-d86b-4052-fc27-f7e2d47947a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>godlessgirl add none want shopping eating toast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>madirigh damn sucks gotta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idk tell girl shut like thunder woke</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>terilynns amazing book school tommorow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>totzhatz sometimes late tired chill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>lab work best massage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>chinese fooood mmmmm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>leeboardman cheers follow anyways</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>diner guess wrong smile god little bit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>maynardcomau boy see done homework vain exist...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  LABEL\n",
       "0     godlessgirl add none want shopping eating toast       0\n",
       "1                           madirigh damn sucks gotta       0\n",
       "2                idk tell girl shut like thunder woke       1\n",
       "3              terilynns amazing book school tommorow       1\n",
       "4                 totzhatz sometimes late tired chill       0\n",
       "..                                                 ...    ...\n",
       "245                             lab work best massage       1\n",
       "246                              chinese fooood mmmmm       1\n",
       "247                 leeboardman cheers follow anyways       1\n",
       "248            diner guess wrong smile god little bit       1\n",
       "249   maynardcomau boy see done homework vain exist...      0\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dNr4A3uvsCuU",
   "metadata": {
    "id": "dNr4A3uvsCuU"
   },
   "outputs": [],
   "source": [
    "best_positive = best_positive.drop(['scores'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39Bfd84OsCrd",
   "metadata": {
    "id": "39Bfd84OsCrd"
   },
   "outputs": [],
   "source": [
    "best_positive = best_positive.drop(['compound'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "td5ajEJSsCpJ",
   "metadata": {
    "id": "td5ajEJSsCpJ"
   },
   "outputs": [],
   "source": [
    "best_positive = best_positive.drop(['comp_score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "MwiC8cx7sCdK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "MwiC8cx7sCdK",
    "outputId": "3d2b5df9-dcef-49a4-eb0f-bfac62d3cdc4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ahhhaaaa win good week rafting awesome amazing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elliematthewson complete legend sad okay well...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>djmiles live support offline amp talked killed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>overun phone ideas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shaunmichaelb summer needs better soon din ro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>jordanknight need songwriters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>rainbowsleeve thanks mike well twit around real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>eastcoastgamblr like drugs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>wash whip beaver nuggets cooked terrific last</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>vacation vacation almost jumped fire keep mind</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  LABEL\n",
       "0      ahhhaaaa win good week rafting awesome amazing       1\n",
       "1     elliematthewson complete legend sad okay well...      0\n",
       "2      djmiles live support offline amp talked killed       0\n",
       "3                                  overun phone ideas       1\n",
       "4     shaunmichaelb summer needs better soon din ro...      1\n",
       "..                                                 ...    ...\n",
       "245                     jordanknight need songwriters       1\n",
       "246   rainbowsleeve thanks mike well twit around real       1\n",
       "247                        eastcoastgamblr like drugs       1\n",
       "248     wash whip beaver nuggets cooked terrific last       1\n",
       "249    vacation vacation almost jumped fire keep mind       0\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "BIgxIk96sSs3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "BIgxIk96sSs3",
    "outputId": "81f3f1ca-e2cd-4965-e614-fb5c60b8f77e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>godlessgirl add none want shopping eating toast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>madirigh damn sucks gotta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idk tell girl shut like thunder woke</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>terilynns amazing book school tommorow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>totzhatz sometimes late tired chill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>jordanknight need songwriters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>rainbowsleeve thanks mike well twit around real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>eastcoastgamblr like drugs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>wash whip beaver nuggets cooked terrific last</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>vacation vacation almost jumped fire keep mind</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  LABEL\n",
       "0     godlessgirl add none want shopping eating toast       0\n",
       "1                           madirigh damn sucks gotta       0\n",
       "2                idk tell girl shut like thunder woke       1\n",
       "3              terilynns amazing book school tommorow       1\n",
       "4                 totzhatz sometimes late tired chill       0\n",
       "..                                                 ...    ...\n",
       "245                     jordanknight need songwriters       1\n",
       "246   rainbowsleeve thanks mike well twit around real       1\n",
       "247                        eastcoastgamblr like drugs       1\n",
       "248     wash whip beaver nuggets cooked terrific last       1\n",
       "249    vacation vacation almost jumped fire keep mind       0\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames = [best_negative, best_positive]  \n",
    "result = pd.concat(frames)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "uKR2aIrfjE4C",
   "metadata": {
    "id": "uKR2aIrfjE4C"
   },
   "outputs": [],
   "source": [
    "result['sentence'] = result['sentence'].apply(lambda s: text_cleaner(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fNusg5Q2hXso",
   "metadata": {
    "id": "fNusg5Q2hXso"
   },
   "outputs": [],
   "source": [
    "result.to_csv(\"C:/Users/HP/Downloads/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "qqSVQZ-NsSnS",
   "metadata": {
    "id": "qqSVQZ-NsSnS"
   },
   "outputs": [],
   "source": [
    "data = data.drop(['DATE_TIME'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4-U6nup3sSMK",
   "metadata": {
    "id": "4-U6nup3sSMK"
   },
   "outputs": [],
   "source": [
    "data = data.drop(['TEXT'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "W_dKMIGctH9-",
   "metadata": {
    "id": "W_dKMIGctH9-"
   },
   "outputs": [],
   "source": [
    "data.rename(columns = {'new':'sentence'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3GoRF7DmtH7z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "3GoRF7DmtH7z",
    "outputId": "ab891606-82fa-46df-a361-39f326303ff8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>get threaded scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>awaisnaseer like shezan mangooo one yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>worked car work showering going bed sooooooooo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>marama actually start afternoon try something ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>gfalcone aww worry vote non stop coz love much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>1</td>\n",
       "      <td>qandq performances clep tests qshock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>0</td>\n",
       "      <td>ugh rcn true blood episodes demand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>1</td>\n",
       "      <td>returned forest sarah merchy lost keys woods day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>1</td>\n",
       "      <td>proud dad piece work keep papa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>0</td>\n",
       "      <td>woke gonna eat pizza breakfast also dentist ap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4287 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LABEL                                           sentence\n",
       "0         0                                get threaded scared\n",
       "1         1      awaisnaseer like shezan mangooo one yesterday\n",
       "2         1  worked car work showering going bed sooooooooo...\n",
       "3         1  marama actually start afternoon try something ...\n",
       "4         1     gfalcone aww worry vote non stop coz love much\n",
       "...     ...                                                ...\n",
       "4282      1               qandq performances clep tests qshock\n",
       "4283      0                 ugh rcn true blood episodes demand\n",
       "4284      1   returned forest sarah merchy lost keys woods day\n",
       "4285      1                     proud dad piece work keep papa\n",
       "4286      0  woke gonna eat pizza breakfast also dentist ap...\n",
       "\n",
       "[4287 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "reeyuhLTtH39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "reeyuhLTtH39",
    "outputId": "80186aec-ad10-4a6e-d353-1eb956a79f93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>godlessgirl add none want shopping eating toast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>madirigh damn sucks gotta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idk tell girl shut like thunder woke</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>terilynns amazing book school tommorow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>totzhatz sometimes late tired chill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>qandq performances clep tests qshock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>ugh rcn true blood episodes demand</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>returned forest sarah merchy lost keys woods day</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>proud dad piece work keep papa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>woke gonna eat pizza breakfast also dentist ap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4787 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  LABEL\n",
       "0       godlessgirl add none want shopping eating toast      0\n",
       "1                             madirigh damn sucks gotta      0\n",
       "2                  idk tell girl shut like thunder woke      1\n",
       "3                terilynns amazing book school tommorow      1\n",
       "4                   totzhatz sometimes late tired chill      0\n",
       "...                                                 ...    ...\n",
       "4282               qandq performances clep tests qshock      1\n",
       "4283                 ugh rcn true blood episodes demand      0\n",
       "4284   returned forest sarah merchy lost keys woods day      1\n",
       "4285                     proud dad piece work keep papa      1\n",
       "4286  woke gonna eat pizza breakfast also dentist ap...      0\n",
       "\n",
       "[4787 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames1 = [result,data]  \n",
    "result1 = pd.concat(frames1)\n",
    "display(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a72dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.to_csv(\"datasetb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6da8cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"datasetb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de09555b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>godlessgirl add none want shopping eating toast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>madirigh damn sucks gotta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>idk tell girl shut like thunder woke</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>terilynns amazing book school tommorow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>totzhatz sometimes late tired chill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>4282</td>\n",
       "      <td>qandq performances clep tests qshock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>4283</td>\n",
       "      <td>ugh rcn true blood episodes demand</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4784</th>\n",
       "      <td>4284</td>\n",
       "      <td>returned forest sarah merchy lost keys woods day</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>4285</td>\n",
       "      <td>proud dad piece work keep papa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4786</th>\n",
       "      <td>4286</td>\n",
       "      <td>woke gonna eat pizza breakfast also dentist ap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4787 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           sentence  LABEL\n",
       "0              0    godlessgirl add none want shopping eating toast      0\n",
       "1              1                          madirigh damn sucks gotta      0\n",
       "2              2               idk tell girl shut like thunder woke      1\n",
       "3              3             terilynns amazing book school tommorow      1\n",
       "4              4                totzhatz sometimes late tired chill      0\n",
       "...          ...                                                ...    ...\n",
       "4782        4282               qandq performances clep tests qshock      1\n",
       "4783        4283                 ugh rcn true blood episodes demand      0\n",
       "4784        4284   returned forest sarah merchy lost keys woods day      1\n",
       "4785        4285                     proud dad piece work keep papa      1\n",
       "4786        4286  woke gonna eat pizza breakfast also dentist ap...      0\n",
       "\n",
       "[4787 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18009e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataset B contains A+500 (result1 here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U45J0_jSXTzY",
   "metadata": {
    "id": "U45J0_jSXTzY"
   },
   "source": [
    "# **Part Evaluation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ApsMYTBLtHzC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApsMYTBLtHzC",
    "outputId": "f6940a07-05ba-4ca8-d6f8-99ee08954ee3"
   },
   "outputs": [],
   "source": [
    "# On Dataset B using CountVectorizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "MNB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "JnjiINBFtoL1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnjiINBFtoL1",
    "outputId": "612a80c2-739d-42dc-8d7d-31d17a3a5827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score_mnb = 69.59%\n"
     ]
    }
   ],
   "source": [
    "# On Dataset B using TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "text_count_2 = tfidf.fit_transform(result1['sentence'])\n",
    "\n",
    "#splitting the data in test and training\n",
    "#from sklearn.model_selection() import train_test_split()\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_2, result1['LABEL'],test_size=0.25,random_state=5)\n",
    "\n",
    "#defining the model\n",
    "#compilimg the model -> we are going to use already used models GNB, MNB, CNB, BNB\n",
    "#fitting the model\n",
    "MNB.fit(x_train, y_train)\n",
    "accuracy_score_mnb = metrics.accuracy_score(MNB.predict(x_test), y_test)\n",
    "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2vDNExkntoJh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vDNExkntoJh",
    "outputId": "1da19a83-3376-4816-a118-9775307885e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.22%\n"
     ]
    }
   ],
   "source": [
    "# On Test data with training on Dataset B using TfidfVectorizer\n",
    "test_text_count_2 = tfidf.transform(test_data['new'])\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "# Fitting on all of the training data present\n",
    "MNB.fit(text_count_2, result1['LABEL'])\n",
    "predicted = MNB.predict(test_text_count_2)\n",
    "accuracy_score = metrics.accuracy_score(predicted, test_data['LABEL'])\n",
    "print(str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "D-7khi4Lf_ez",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-7khi4Lf_ez",
    "outputId": "2a071e48-24f6-4cfb-a3f5-07ce08757a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score_mnb = 69.59%\n"
     ]
    }
   ],
   "source": [
    "# On Dataset A using TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "text_count_2 = tfidf.fit_transform(data['sentence'])\n",
    "\n",
    "#splitting the data in test and training\n",
    "#from sklearn.model_selection() import train_test_split()\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_2, data['LABEL'],test_size=0.25,random_state=5)\n",
    "\n",
    "#defining the model\n",
    "#compilimg the model -> we are going to use already used models GNB, MNB, CNB, BNB\n",
    "#fitting the model\n",
    "model1 = MNB.fit(x_train, y_train)\n",
    "accuracy_score_mnb = metrics.accuracy_score(MNB.predict(x_test), y_test)\n",
    "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "_afa5GEff_ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_afa5GEff_ce",
    "outputId": "667c8ab6-9313-4d1e-9186-b380fccdb18c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.24%\n"
     ]
    }
   ],
   "source": [
    "# On Test data with training on Dataset A using TfidfVectorizer\n",
    "test_text_count_2 = tfidf.transform(test_data['new'])\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "# Fitting on all of the training data present\n",
    "MNB.fit(text_count_2, data['LABEL'])\n",
    "predicted = MNB.predict(test_text_count_2)\n",
    "accuracy_score = metrics.accuracy_score(predicted, test_data['LABEL'])\n",
    "print(str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0qY-DzsLgLTR",
   "metadata": {
    "id": "0qY-DzsLgLTR"
   },
   "outputs": [],
   "source": [
    "import dill as pickle \n",
    "with open('a2_naivebayes_model.pkl', 'wb') as fout:\n",
    "    pickle.dump(MNB, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0uglcHlogLRA",
   "metadata": {
    "id": "0uglcHlogLRA"
   },
   "outputs": [],
   "source": [
    "with open('a2_naivebayes_model.pkl', 'rb') as fin:\n",
    "    ngram_loaded = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b3ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855edfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "D6CjLdypdyBJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6CjLdypdyBJ",
    "outputId": "63f59b9c-6bc3-4aac-bc69-29f075956802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------BIGRAM RANDOMLY GENERATE SENTENCES TEST--------\n",
      "perplexity score of 5 sentences: 47.94826475553644\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------BIGRAM RANDOMLY GENERATE SENTENCES TEST--------\")\n",
    "bigram_sentences = ngram.bigram_generate_sentences(50)\n",
    "ngram.N = 2\n",
    "for sentence in bigram_sentences:\n",
    "    joined_sentence = \" \".join(sentence)\n",
    "    score = ngram.prob(joined_sentence)\n",
    "    smoothed_score = ngram.sprob(joined_sentence)\n",
    "    #print(joined_sentence, \":\", score, smoothed_score)\n",
    "#ppl_score = ngram.ppl(bigram_sentences)\n",
    "#ppl_score_with_smoohting = ngram.ppl_unigram_smoohted(bigram_sentences)\n",
    "bigram_ppl_score = ngram.ppl_bigram(bigram_sentences)\n",
    "    \n",
    "print(\"perplexity score of 5 sentences:\",bigram_ppl_score) #ppl_score_with_smoohting, ppl_score,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "r--cUeIOdx-g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r--cUeIOdx-g",
    "outputId": "66d5aa68-35e3-4c08-ad54-4dcf0a98a292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------BIGRAM TEST--------\n",
      "The perplexity is: 1.0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"about to bat him\"\n",
    "sentence = preprocess(sentence)\n",
    "print(\"\\n------BIGRAM TEST--------\")\n",
    "ngram.N = 2\n",
    "print(\"The perplexity is:\",ngram.ppl_bigram(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "F5x4Zi7Bmoih",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5x4Zi7Bmoih",
    "outputId": "6ee72048-06d0-40a5-ef18-9adda95d8f68",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pandas as pd\\ndf = pd.read_csv('C:/Users/HP/Downloads/good_turing_smooting_bigram_result.txt', sep=':',header=None)\\nprint(df)\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "df = pd.read_csv('C:/Users/HP/Downloads/good_turing_smooting_bigram_result.txt', sep=':',header=None)\n",
    "print(df)'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
